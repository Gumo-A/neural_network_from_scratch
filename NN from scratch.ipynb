{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151972de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7161b056",
   "metadata": {},
   "source": [
    "# Building a neural network from scratch\n",
    "This is a personal project, my intent is to get a better understanding of NN and how they \"learn\".\n",
    "\n",
    "## What is a neural network?\n",
    "\n",
    "Neural networks are a type of algorithm inspired by the way brains work. They are constructed by three main parts, the __layers__: \n",
    "* The input layer: The data we'll feed to the NN\n",
    "* The hidden layers: A collection of functions that process the data, where the network finds paterns\n",
    "* The output layer: The value or class we are trying to predict\n",
    "\n",
    "Here's a definition I took from IBM's website: \n",
    "\n",
    "_Neural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of machine learning and are at the heart of deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that biological neurons signal to one another._\n",
    "\n",
    "_Artificial neural networks (ANNs) are comprised of node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87ace79",
   "metadata": {},
   "source": [
    "## Nodes and Weights\n",
    "__Nodes__ are what constitude each layer. Each node in any given layer is linked with all of the next layer's nodes. Think about it this way: every two layers in a NN is a fully conected directed bipartite network. These links come in the form of what we call __weights__, which are a number that is multiplied by the link's parent node value. So the receiving node has three values, each one is the value of the previous node multiplied by the weight, and they are all summed up, plus a bias, to form the value of this node.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370237ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "# Here, 7, 3 and 2 are individual nodes\n",
    "input_layer = [7, 3, 2]\n",
    "# These are the values each node will be multiplied by\n",
    "weights = [0.1, -0.6, 0.5]\n",
    "# The bias will be added at the end\n",
    "bias = 1\n",
    "\n",
    "new_node = (input_layer[0]*weights[0] + input_layer[1]*weights[1] + input_layer[2]*weights[2]) + bias\n",
    "print(round(new_node, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68387dcf",
   "metadata": {},
   "source": [
    "## Computing the value of nodes\n",
    "The purose of the following code is to make the calculations of the next layer easy to see. Further down we'll start using Numpy to run the code faster with less typing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b458442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the same, but this time with four nodes in the input layer (HL) and three nodes in the output layer. \n",
    "# We will simulate the output layer, coming from the last hidden layer. \n",
    "\n",
    "HL = [7, 3, 2, 3]\n",
    "\n",
    "# Weights for the first node\n",
    "W1 = [0.1, -0.6, 0.5, 0.8]\n",
    "# For the second\n",
    "W2 = [0.7, 0.4, 0.1, -0.6]\n",
    "# For the third\n",
    "W3 = [0.5, -0.6, -0.8, 0.13]\n",
    "\n",
    "bias1 = 5\n",
    "bias2 = 7\n",
    "bias3 = 5\n",
    "\n",
    "output = [HL[0]*W1[0] + HL[1]*W1[1] + HL[2]*W1[2] + HL[3]*W1[3] + bias1,\n",
    "         HL[0]*W2[0] + HL[1]*W2[1] + HL[2]*W2[2] + HL[3]*W2[3] + bias2,\n",
    "         HL[0]*W3[0] + HL[1]*W3[1] + HL[2]*W3[2] + HL[3]*W3[3] + bias3]\n",
    "\n",
    "print(list(map(lambda x: round(x,2), output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2335f820",
   "metadata": {},
   "source": [
    "Tweaking the weights and the biases is how the NN is able to generalize and make decisions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b5e3a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lets clean the previous code\n",
    "\n",
    "HL = np.array([7, 3, 2, 3])\n",
    "\n",
    "weights = np.array([[0.1, -0.6, 0.5, 0.8],\n",
    "          [0.7, 0.4, 0.1, -0.6],\n",
    "          [0.5, -0.6, -0.8, 0.13]])\n",
    "\n",
    "biases = np.array([5, 7, 5])\n",
    "\n",
    "output = []\n",
    "# Loop through the weights and biases corresponding to each output node\n",
    "# This first loop runs for 3 iterations, one for every output node\n",
    "for weight, bias in zip(weights, biases):\n",
    "    node_output = 0\n",
    "    # Loop through the inputs and the weight of this loop's node\n",
    "    # Compute the product of each input by its weight and add the result to the node\n",
    "    for n_input, weight1 in zip(HL, weight):\n",
    "        node_output += n_input*weight1\n",
    "    # Add the bias to the node and append it to the list\n",
    "    node_output += bias\n",
    "    output.append(node_output)\n",
    "    # Repeat for the rest of the nodes\n",
    "    \n",
    "list(map(lambda x: round(x,2), output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedbe1de",
   "metadata": {},
   "source": [
    "### Batches\n",
    "We can speed up the computing time by taking advantage of computers' capacity to compute in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d771a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[7, 3, 2, 3],\n",
    "                   [3, 6, 9, 1],\n",
    "                   [3, 7, 1, 9]])\n",
    "\n",
    "weights = np.array([[0.1, -0.6, 0.5, 0.8],\n",
    "                    [0.7, 0.4, 0.1, -0.6],\n",
    "                    [0.5, -0.6, -0.8, 0.13]])\n",
    "\n",
    "biases = [5, 7, 5]\n",
    "# The commented out line of code won't work because the rows of inputs doesnt align with the columns of weights\n",
    "#np.dot(inputs, weights)\n",
    "# We need to compute each row of input to the weights, so we have to transpose the input matrix for this to work\n",
    "\n",
    "np.dot(inputs, weights.T)+biases\n",
    "# Now each row corresponds to an output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda5776b",
   "metadata": {},
   "source": [
    "This new matrix can then be fed to the next layer, we can do the same operations using new weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abec9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "inputs1 = np.array([[7, 3, 2, 3],\n",
    "                   [3, 6, 9, 1],\n",
    "                   [3, 7, 1, 9]])\n",
    "\n",
    "weights1 = np.array([[0.1, -0.6, 0.5, 0.8],\n",
    "                    [0.7, 0.4, 0.1, -0.6],\n",
    "                    [0.5, -0.6, -0.8, 0.13]])\n",
    "\n",
    "biases1 = [5, 7, 5]\n",
    "\n",
    "inputs2 = np.random.randn(3,4)\n",
    "\n",
    "weights2 = np.random.randn(3,4)\n",
    "\n",
    "biases2 = np.random.randint(0, 9, 3)\n",
    "\n",
    "first_layer = np.dot(inputs1, weights1.T)+biases1\n",
    "print(first_layer.shape)\n",
    "second_layer = np.dot(inputs2, weights2.T)+biases2\n",
    "second_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b78135",
   "metadata": {},
   "source": [
    "## Objects\n",
    "It'll be a lot easier to work with the network if we create objects. This part of the notebook I took from Sentex's YouTube tutorial on Neural Networks from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff64977",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X = np.array([[1, 2, 3, 2.5],\n",
    "              [2, 5, -1, 2],\n",
    "              [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "class Layer_Dense:\n",
    "    # We'll need to define what are the number of inputs \n",
    "    # and what are the number of neurons they are conected to\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # multiply by 0.1 to keep values small\n",
    "        self.weights = 0.1*np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# The first parameter is the number of inputs to this layer\n",
    "# the second is the number of neurons we want to create (any number we want)\n",
    "layer1 = Layer_Dense(X.shape[1], 5)\n",
    "layer1.forward(X)\n",
    "print(layer1.output, '\\n')\n",
    "\n",
    "# We can then use the output of the first step as the input of the second\n",
    "# We just need to provide the number of inputs (through the shape of the last output),\n",
    "# and the number of neurons we want (any number, 2 in this case)\n",
    "layer2 = Layer_Dense(layer1.output.shape[1], 2)\n",
    "layer2.forward(layer1.output)\n",
    "print(layer2.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a92aee",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "They are functions that reside in every node of the hidden layers. Their job is to output certain values, given a certain input. In this first example, we'll use a _step function_, which just outputs 0s and Xs (original values), depending on wether the input is > 0 or < 0. Output layers usually have a different activation function. Other types of activation functions: sigmoid function, rectified linear function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8a6562",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    # This is all there is to the activation function. \n",
    "    # If it's zero or less, output 0. If it's more than 0, output the input.\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f321e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = Layer_Dense(4, 5)\n",
    "activation1 = Activation_ReLU()\n",
    "layer1.forward(X)\n",
    "print(layer1.output, '\\n')\n",
    "activation1.forward(layer1.output)\n",
    "print(activation1.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfc8680",
   "metadata": {},
   "source": [
    "## SoftMax Activation function  \n",
    "We have to determine how wrong each prediction is. The way we have done things so far makes the outputs susceptible to be greater than 1. We could, for example, have [1.5, 4.6, 0.89] as the outputs. And of course, we could infer from this that the algorithm has decided that the second output is the more probable, but in order to make backpropagation work, we need to have standardized outputs, because we need to calculate a loss function at the end of every iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a926e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "import math\n",
    "\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "# Get rid of negative values\n",
    "e = math.e\n",
    "exp_outputs = [e**i for i in layer_outputs]\n",
    "\n",
    "# Scale to \"probabilities\"\n",
    "summed = sum(exp_outputs)\n",
    "output = [i/summed for i in exp_outputs]\n",
    "print(f'With raw Python: {output}')\n",
    "\n",
    "# With numpy\n",
    "\n",
    "exp_outputs = np.exp(layer_outputs)\n",
    "output = exp_outputs/np.sum(exp_outputs)\n",
    "# These two lines of code is what we call the SoftMax activation function\n",
    "\n",
    "print(f'With numpy: {output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca30a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [[4.8, 1.21, 2.385],\n",
    "                [8.9, -1.81, 0.2],\n",
    "                [1.41, 1.051, 0.026]]\n",
    "\n",
    "exp_outputs = np.exp(layer_outputs)\n",
    "\n",
    "# To be able to sum the rows of the exp_outputs matrix, we can tweak the parameters of np.sum()\n",
    "print(np.sum(exp_outputs, axis=1))\n",
    "x = exp_outputs/np.sum(exp_outputs, axis=1)\n",
    "print(f'Sums: {np.sum(x, axis=1)}', '\\n')\n",
    "\n",
    "# But that gives out a vector which sum doesnt equal 1. We would like to keep the sums in their rows so \n",
    "# we can divide the exp_outputs matrix by the sums automaticaly like before\n",
    "print(np.sum(exp_outputs, axis=1, keepdims=True))\n",
    "x = exp_outputs/np.sum(exp_outputs, axis=1, keepdims=True)\n",
    "print(f'Sums: {np.sum(x, axis=1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041e41ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid overflow erros, where the number being computed is too large\n",
    "# we can substract all the numbers of the input by the largest input.\n",
    "# This will cause the exp function to get fixed between 0 and 1,\n",
    "# and the result doesnt change because we apply the same operation to every input.\n",
    "\n",
    "# We'll do this as we create the SoftMax object\n",
    "\n",
    "class SoftMax:\n",
    "    def forward(self, inputs):\n",
    "        # np.max(inputs) would output a single number\n",
    "        # Tweaked the parameters just as before\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values/np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fa47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a module Sentex created to generate data\n",
    "\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "\n",
    "X, y = spiral_data(samples=100, classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598329b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    # We'll need to define what are the number of inputs \n",
    "    # and what are the number of neurons they are conected to\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # multiply by 0.1 to keep values small\n",
    "        self.weights = 0.1*np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "class Activation_ReLU:\n",
    "    # This is all there is to the activation function. \n",
    "    # If it's zero or less, output 0. If it's more than 0, output the input.\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    \n",
    "class SoftMax:\n",
    "    def forward(self, inputs):\n",
    "        # np.max(inputs) would output a single number\n",
    "        # Tweaked the parameters just as before\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values/np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372f4aba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dense1 = Layer_Dense(2, 3)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "activation2 = SoftMax()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "dense1.output[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514a8fc2",
   "metadata": {},
   "source": [
    "## The loss function: Categorical cross-entropy\n",
    "This function takes the natural logarithm of the predicted probabilities of each class and multiplies it by the corresponding One-Hot encoded vector value (see the example below for clarification) and adds them up.\n",
    "\n",
    "Since the only non-zero value of the encoding is the actual class, the formula for cross-entropy simplyfies a lot: it is the negative log of the highest predicted probability for each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60aac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say the predicted class is the first one\n",
    "\n",
    "predictions = [0.7, 0.25, 0.05]\n",
    "\n",
    "OneHot_Encoding = [1, 0, 0]\n",
    "\n",
    "CrossEntropyLoss = [-math.log(predictions[0])*OneHot_Encoding[0],\n",
    "                   -math.log(predictions[1])*OneHot_Encoding[1],\n",
    "                   -math.log(predictions[2])*OneHot_Encoding[2]]\n",
    "print(sum(CrossEntropyLoss))\n",
    "# Notice this is the same thing as \n",
    "print(-math.log(predictions[0]), '\\n')\n",
    "# Because the rest just cancels out\n",
    "# Also, notice how the loss increases as the predicted value is more \"wrong\"\n",
    "for i in predictions:\n",
    "    print(-math.log(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb87055",
   "metadata": {},
   "source": [
    "The loss increases exponentially as the predicted value approaches 0. This will help us penalize weights and biases that are performing poorly even more than mearly taking 1-p. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d78059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,1, 1000)\n",
    "x[0] = 0.00001\n",
    "y = [-math.log(i) for i in x]\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(x, y, label='Cross-Entropy')\n",
    "plt.plot(x, [1-i for i in x], label='1-p')\n",
    "plt.xlabel('Predictions')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Cross-Entropy compared to 1-p as a loss function')\n",
    "plt.annotate('Cross-Entropy excels at penalizing the worst predictions', xy=(0.2,7), size='large')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69983821",
   "metadata": {},
   "source": [
    "## Implementing the loss function\n",
    "We are passing a batch of samples to the NN, so we will also have a batch of predictions at the end of each iteration, and that batch we'll need to evaluate against a batch of _actual_ classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5db21c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say this is our output, one line for each sample\n",
    "\n",
    "output = np.array([[0.7, 0.1, 0.2],\n",
    "                   [0.1, 0.5, 0.4],\n",
    "                   [0.02, 0.9, 0.08]])\n",
    "\n",
    "# And let's say this is our y, our actual class vector\n",
    "y = np.array([0, 1, 1])\n",
    "# Where each value corresponds to the actual class of the row of our output\n",
    "# In our output:\n",
    "# the first row's actual value is 0: so the value we want is 0.7\n",
    "# the second row's actual value is 1: so we want 0.5\n",
    "# and the third row's actual value is 1: so we want 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa2994",
   "metadata": {},
   "source": [
    "So in our example, the desired result would be [0.7, 0.5, 0.9]; the confidence of the model for the actual classes.\n",
    "We could write a loop to use the values in \"y\" as the index we need from each line of our output matrix, but it'd be faster and more compact to do this with numpy.\n",
    "\n",
    "All we have to do is use lists as the indeces for rows and columns. We can iterate through all the rows and pass the \"y\" vector as the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71edd833",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[[0, 1, 2], [0, 1, 1]])\n",
    "\n",
    "# Same as\n",
    "\n",
    "print(output[[0, 1, 2], y])\n",
    "\n",
    "# Same as\n",
    "\n",
    "print(output[range(len(output)), y])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b95d6c",
   "metadata": {},
   "source": [
    "Now we just need to pass these values through our loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59a5c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "-np.log(output[\n",
    "    range(len(output)),\n",
    "    y\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e52229",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "This is the messier part of my notebook so far. Backpropagation is very dense, both theorically and practically. I spent a week watching tutorials, reading, and writting down the derivates of each of the functions of various NN architectures. \n",
    "\n",
    "This is where I had to stop, because my semester is starting soon and I need to focus. But I want to finish this project, seeing the loss function converge to near zero was one of the most satisfing things I have ever experienced. \n",
    "\n",
    "My implementation is certainly not optimal, and my loss function has weird spikes as it goes down to the optimum value. But I did it on my own, and got a very good look into the inner workings of Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b521dc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    # We'll need to define what are the number of inputs \n",
    "    # and what are the number of neurons they are conected to\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # multiply by 0.1 to keep values small\n",
    "        self.weights = 0.1*np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "class Activation_ReLU:\n",
    "    # This is all there is to the activation function. \n",
    "    # If it's zero or less, output 0. If it's more than 0, output the input.\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "class SoftMax:\n",
    "    def forward(self, inputs):\n",
    "        # np.max(inputs) would output a single number\n",
    "        # Tweaked the parameters just as before\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values/np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "class Activation_Sigmoid:\n",
    "    def forward(self, inputs):\n",
    "        self.output = 1/(1+np.exp(-inputs))\n",
    "        \n",
    "\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        self.sample_loss = self.forward(output, y)\n",
    "        mean_loss = np.mean(self.sample_loss)\n",
    "        return mean_loss\n",
    "    \n",
    "# CCE = Categorical cross-entropy\n",
    "class Loss_CCE(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        # We can have actual values in the form of one-hot encoded list of lists\n",
    "        # or we could have just one list with the value for each class\n",
    "        # We need to take both formats in consideration\n",
    "        \n",
    "        # If y_true is just a vector\n",
    "        # This is the case we worked on, in the previous example\n",
    "        if len(y_true.shape) == 1:\n",
    "            self.correct_confidence = y_pred_clipped[range(len(y_pred_clipped)), y_true]\n",
    "        # This is a matrix\n",
    "        # One-Hot encoded\n",
    "        elif len(y_true.shape) == 2:\n",
    "            # Element-wise multiplication of the 2 matrices\n",
    "            # y_true has one row of 0s and a 1 for every sample\n",
    "            # So the multiplication only occurs on the predicted value of the actual class\n",
    "            # Then np.sum() \"sums\" (there is only one non-zero value) the values along the rows\n",
    "            self.correct_confidence = np.sum(y_pred_clipped*y_true, axis=1)\n",
    "        else: correct_confidence = y_pred[0][y_true]\n",
    "        \n",
    "        negative_log_likelihoods = -np.log(self.correct_confidence)\n",
    "        return negative_log_likelihoods            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f32f26f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create data and pass it through the untrained NN\n",
    "np.random.seed(0)\n",
    "X, y = spiral_data(samples=100, classes=2)\n",
    "\n",
    "dense1 = Layer_Dense(2, 2)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(2, 2)\n",
    "activation2 = SoftMax()\n",
    "\n",
    "loss_function = Loss_CCE()\n",
    "\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0384167",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adae201d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "samples = 100\n",
    "classes = 3\n",
    "m = samples*classes\n",
    "\n",
    "X, y = spiral_data(samples=samples, classes=classes)\n",
    "\n",
    "dense1 = Layer_Dense(2, 10)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(10, classes)\n",
    "activation2 = SoftMax()\n",
    "\n",
    "loss_function = Loss_CCE()\n",
    "####################\n",
    "losses = []\n",
    "alpha = 0.1\n",
    "for i in range(3000):\n",
    "\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "    OH_y = np.zeros(activation2.output.shape)\n",
    "    OH_y[range(len(activation2.output)), y] = 1\n",
    "    \n",
    "    dSM = activation2.output - OH_y\n",
    "    dw2 = np.dot(activation1.output.T, dSM)/m\n",
    "    da1 = np.dot(dense2.weights, dSM.T)*np.greater(activation1.output, 0).astype(int).T\n",
    "    dw1 = np.dot(X.T, da1.T)/m\n",
    "    \n",
    "    b2 = np.subtract(dense2.biases, (alpha*dSM.mean(axis=0)))\n",
    "    w2 = np.subtract(dense2.weights, (alpha*dw2))\n",
    "    b1 = np.subtract(dense1.biases, (alpha*da1.mean(axis=1, keepdims=True).T))\n",
    "    w1 = np.subtract(dense1.weights, (alpha*dw1))\n",
    "\n",
    "    dense2.biases = b2\n",
    "    dense2.weights = w2\n",
    "    dense1.biases = b1\n",
    "    dense1.weights = w1\n",
    "\n",
    "    losses.append(loss)\n",
    "    if i%10000 == 0:\n",
    "        print(f'Loss on the {i}th iteration: {loss}')\n",
    "print(loss)\n",
    "plt.plot(range(len(losses)), losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcdb9a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "s = 200\n",
    "c = 2\n",
    "m = s*c\n",
    "\n",
    "X, y = spiral_data(samples=s, classes=c)\n",
    "\n",
    "dense1 = Layer_Dense(2, 100)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(100, 100)\n",
    "activation2 = Activation_ReLU()\n",
    "dense3 = Layer_Dense(100, c)\n",
    "activation3 = SoftMax()\n",
    "\n",
    "loss_function = Loss_CCE()\n",
    "####################\n",
    "losses = []\n",
    "alpha = 0.5\n",
    "for i in range(3000):\n",
    "\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "    dense3.forward(activation2.output)\n",
    "    activation3.forward(dense3.output)\n",
    "\n",
    "    loss = loss_function.calculate(activation3.output, y)\n",
    "\n",
    "    OH_y = np.zeros(activation3.output.shape)\n",
    "    OH_y[range(len(OH_y)), y] = 1\n",
    "    \n",
    "    dSM = activation3.output - OH_y\n",
    "    da2 = np.dot(dense3.weights, dSM.T)*np.greater(activation2.output, 0).astype(int).T\n",
    "    da1 = np.dot(dense2.weights, da2)*np.greater(activation1.output, 0).astype(int).T\n",
    "    dw3 = np.dot(activation2.output.T, (dSM))/m\n",
    "    dw2 = np.dot(activation1.output.T, da2.T)/m\n",
    "    dw1 = np.dot(da1, X).T/m\n",
    "    \n",
    "    b3 = np.subtract(dense3.biases, (alpha*dSM.mean(axis=0)))\n",
    "    w3 = np.subtract(dense3.weights, (alpha*dw3))\n",
    "    b2 = np.subtract(dense2.biases, (alpha*da2.mean(axis=1)))\n",
    "    w2 = np.subtract(dense2.weights, (alpha*dw2))\n",
    "    b1 = np.subtract(dense1.biases, (alpha*da1.mean(axis=1)))\n",
    "    w1 = np.subtract(dense1.weights, (alpha*dw1))\n",
    "    \n",
    "    dense3.biases = b3\n",
    "    dense3.weights = w3\n",
    "    dense2.biases = b2\n",
    "    dense2.weights = w2\n",
    "    dense1.biases = b1\n",
    "    dense1.weights = w1\n",
    "\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if i%1000 == 0:\n",
    "        print(f'Loss on the {i}th iteration: {loss}')\n",
    "        \n",
    "print(loss)\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50a0a9",
   "metadata": {},
   "source": [
    "I still have to figure out why I get those spikes during training, but it is still satisfying to see the NN converge to an optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655af6d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can graph the decision boundary of the trained NN\n",
    "\n",
    "min1, max1 = X[:, 0].min()-.1, X[:, 1].max()+.1\n",
    "min2, max2 = X[:, 0].min()-.1, X[:, 1].max()+.1\n",
    "\n",
    "x1_scale = np.arange(min1, max1, 0.01)\n",
    "x2_scale = np.arange(min2, max2, 0.01)\n",
    "\n",
    "x_grid, y_grid = np.meshgrid(x1_scale, x2_scale)\n",
    "\n",
    "x_g, y_g = x_grid.flatten(), y_grid.flatten()\n",
    "x_g, y_g = x_g.reshape(len(x_g),1), y_g.reshape(len(y_g),1)\n",
    "\n",
    "grid = np.hstack((x_g, y_g))\n",
    "\n",
    "grid1 = np.array([np.arange(-1,1,0.0001), np.arange(-1,1,0.0001)]).T\n",
    "\n",
    "dense1.forward(grid)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "dense3.forward(activation2.output)\n",
    "activation3.forward(dense3.output)\n",
    "\n",
    "op = activation3.output\n",
    "\n",
    "pred = np.greater(op, 0.5)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y)\n",
    "plt.contourf(x_grid, y_grid, pred[:, 1].reshape(x_grid.shape), alpha=0.25);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
